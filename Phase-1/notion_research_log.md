# ì—°êµ¬ ì¼ì§€ - 2026ë…„ 2ì›” 10ì¼ (Week 4, Day 1)

## ğŸ“Œ ì§„í–‰ ìƒí™© ìš”ì•½

**Current Status**: Subtask 1 - ê¸°ë°˜ ë©”íŠ¸ë¦­ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ

**ì™„ë£Œëœ ì‘ì—…**:
- âœ… í”„ë¡œì íŠ¸ êµ¬ì¡° ì¬ì •ë¹„ (legacy í´ë”ë¡œ êµ¬í˜„ ì´ê´€)
- âœ… Khan Academy taxonomy ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- âœ… Domain + Quality ë©”íŠ¸ë¦­ ê³„ì‚° íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- âœ… Interactive dashboard ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì™„ì„±
- âœ… ì „ì²´ ë¬¸ì„œí™” (README.md)

---

## ğŸ¯ ì˜¤ëŠ˜ì˜ í•µì‹¬ ê²°ì •ì‚¬í•­

### 1. Graph ì ‘ê·¼ë²• â†’ Vector ê¸°ë°˜ ì ‘ê·¼ë²•ìœ¼ë¡œ ì „í™˜

**ì´ì „ ê³„íš**: Prerequisite graphë¥¼ ëª…ì‹œì ìœ¼ë¡œ êµ¬ì¶•

**ìƒˆë¡œìš´ ì ‘ê·¼**:
- Embedding similarity ê¸°ë°˜ domain classification
- Multi-label soft assignment (multi-head attention ê°œë… ì°¨ìš©)
- Cross-cutting conceptsë¥¼ í™•ë¥  ë¶„í¬ë¡œ í‘œí˜„

**ì´ìœ **:
- GraphëŠ” prerequisite ê´€ê³„ ëª¨ë¸ë§ì— ìœ ìš©í•˜ì§€ë§Œ, domain coverage ë¶„ì„ì—ëŠ” ê³¼ë„í•˜ê²Œ ë³µì¡
- Vector ê¸°ë°˜ ì ‘ê·¼ì´ ë” scalableí•˜ê³  implementationì´ ê°„ë‹¨
- Multi-domain ë¬¸ì„œë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì²˜ë¦¬ ê°€ëŠ¥ (ì˜ˆ: í•œ ë¬¸ì¥ì´ math 60% + physics 40%)

### 2. ë©”íŠ¸ë¦­ ìš°ì„ ìˆœìœ„: Domain + Quality ë¨¼ì €

**ì„ íƒí•œ ë©”íŠ¸ë¦­**:
1. **Domain Coverage** (Multi-label classification)
   - Khan Academy taxonomyë¥¼ concept prototypesë¡œ í™œìš©
   - Embedding similarityë¡œ soft labels í• ë‹¹
   - Top-5 concepts with normalized probabilities

2. **Quality Metrics**
   - Perplexity (GPT-2): í…ìŠ¤íŠ¸ì˜ ìì—°ìŠ¤ëŸ¬ì›€ ì¸¡ì •
   - Educational markers: examples, explanations, structure ê²€ì¶œ

**ë¯¸ë¤„ì§„ ë©”íŠ¸ë¦­** (Week 5-6ì— ì¶”ê°€):
- Difficulty (Flesch-Kincaid readability score)
- Redundancy (MinHash LSH for near-duplicate detection)

**ì´ìœ **:
- 16ì£¼ ì¤‘ 4ì£¼ì°¨, ì‹œê°„ ì œì•½
- Domain + Qualityë§Œìœ¼ë¡œë„ ì¶©ë¶„í•œ insight í™•ë³´ ê°€ëŠ¥
- ë‚˜ë¨¸ì§€ëŠ” ì ì§„ì  ì¶”ê°€ ê°€ëŠ¥

### 3. ë°ì´í„°ì…‹ í™œìš© ì „ëµ: Khan + Tiny-Textbooks

**Khan Academy ì—­í• **:
- Taxonomy source (structured labels)
- Concept prototype ìƒì„± (embedding baselines)
- Ground truth for validation

**Tiny-Textbooks ì—­í• **:
- Classification ëŒ€ìƒ (unlabeled, high-quality)
- Khan taxonomy ì ìš© ê°€ëŠ¥ì„± ê²€ì¦
- Real-world distribution íŒŒì•…

**The Pile**: Week 7+ ì´í›„ í™•ì¥

---

## ğŸ”¬ êµ¬í˜„ëœ íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜

### Step 1: Taxonomy Extraction (`1_extract_khan_taxonomy.py`)

**Input**: `khan_k12_concepts/all_k12_concepts.json` (982KB, 19 subjects)

**Process**:
```python
Khan Academy concepts
    â†“
Extract hierarchy (Subject â†’ Grade â†’ Concept)
    â†“
Embed each concept's article content
    â†“
Create concept prototypes (384-dim vectors)
```

**Output**:
- `outputs/khan_taxonomy.json` - Hierarchical structure
- `outputs/concept_prototypes.pkl` - Embeddings
- `outputs/metadata.json` - Statistics

**Embedding Model**: SentenceTransformer `all-MiniLM-L6-v2`
- Fast, lightweight (384 dimensions)
- Good balance of speed and quality
- Alternative models commented in code (Instructor, E5)

---

### Step 2: Metrics Computation (`2_compute_metrics.py`)

**Input**:
- Concept prototypes from Step 1
- Khan Academy full dataset
- Tiny-Textbooks (42 batches, ~420K docs)

**Process** (for each paragraph):
```python
Text chunk (200 words)
    â†“
Embed with SentenceTransformer
    â†“
Compute cosine similarity to all concept prototypes
    â†“
Top-5 domains with similarity > 0.3 â†’ soft labels
    â†“
Compute perplexity with GPT-2
    â†“
Detect educational markers (regex patterns)
```

**Configuration**:
- `TOP_K_DOMAINS = 5` - Multi-label assignment
- `MIN_SIMILARITY = 0.3` - Threshold for relevance
- `CHUNK_SIZE = 200` - Words per paragraph

**Output**:
- `outputs/khan_analysis.jsonl` - Khan Academy results
- `outputs/tiny_textbooks_analysis.jsonl` - Tiny-Textbooks results

**Expected Runtime**:
- Khan Academy: ~10-15 minutes
- Tiny-Textbooks (full): ~1-2 hours on dual GPU (4060Ti + 3070Ti)

---

### Step 3: Dashboard Generation (`3_build_dashboard.py`)

**Input**: Analysis results from Step 2

**Aggregations**:
1. Domain distribution (subject-level counts)
2. Top 10 concepts by frequency
3. Quality statistics (mean/median perplexity, marker ratios)
4. Cross-cutting analysis (multi-domain percentage)

**Visualization**:
- Interactive HTML dashboard (Chart.js)
- Subject distribution comparison (bar chart)
- Educational markers comparison (bar chart)
- Top concepts (horizontal bar charts)
- Quality metrics table

**Output**: `outputs/dashboard.html` (self-contained, no server needed)

---

## ğŸ“Š ì˜ˆìƒ ê²°ê³¼ (ê°€ì„¤)

### Domain Coverage

**Khan Academy**:
- âœ… Well-balanced across K-12 subjects (Math, Science, Reading, History)
- âœ… Higher multi-domain ratio (cross-cutting concepts in FAQ format)
- âš ï¸ Sparse in advanced topics (limited to K-12)

**Tiny-Textbooks**:
- âœ… More uniform distribution (GPT-generated diversity)
- âš ï¸ Potential bias toward common/popular topics
- â“ Lower multi-domain ratio? (textbook format = single topic focus)

### Quality Metrics

**Khan Academy**:
- âœ… Lower perplexity (~40-50) - human-written, curated
- âœ… High educational marker prevalence (examples, explanations)
- âœ… Consistent structure (FAQ format)

**Tiny-Textbooks**:
- âš ï¸ Slightly higher perplexity (~50-60) - GPT artifacts
- âœ… High structure consistency (synthetic, templated)
- â“ Fewer examples? (generated vs. human-crafted)

---

## ğŸš§ ë‹¤ìŒ ë‹¨ê³„ (Week 4-5)

### Immediate (This Week)
1. âœ… ì½”ë“œ ì™„ì„± ë° ë¬¸ì„œí™” (DONE)
2. â³ Step 1 ì‹¤í–‰: Taxonomy extraction (~10 minutes)
3. â³ Step 2 ì‹¤í–‰: Metrics computation (test with `max_batches=5` first)
4. â³ Step 3 ì‹¤í–‰: Dashboard generation
5. â³ ê²°ê³¼ ê²€ì¦: Manual inspection of 50-100 classified paragraphs

### Validation Strategy
- Sample 100 paragraphs from each dataset
- Manually label domain
- Compare with model predictions
- Compute precision/recall
- Use GPT-4 as second annotator for inter-annotator agreement

### Next Week (Week 5)
1. Full Tiny-Textbooks processing (all 42 batches)
2. Add difficulty metrics (Flesch-Kincaid)
3. Preliminary analysis write-up
4. Share dashboard with professor for feedback

---

## ğŸ’¡ ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸

### 1. SLM Trainingì€ ê°€ëŠ¥í•˜ë‹¤ (GPU í™•ì¸)
- 4060Ti (16GB) + 3070Ti (8GB) = 24GB VRAM
- 100M ëª¨ë¸ trainingì€ ê°€ëŠ¥ (LoRA/QLoRA í™œìš© ì‹œ)
- 300Mì€ tightí•˜ì§€ë§Œ gradient checkpointingìœ¼ë¡œ ê°€ëŠ¥

### 2. Dataset Characterizationì˜ Novel Contribution
ê¸°ì¡´ ì—°êµ¬ (Dolma, FineWeb, DataComp):
- âŒ High-level domainë§Œ ë¶„ë¥˜ (web/books/code)
- âŒ Fine-grained taxonomy ì—†ìŒ
- âŒ Cross-cutting concepts ë¯¸ë¶„ì„

ìš°ë¦¬ ì ‘ê·¼:
- âœ… Fine-grained taxonomy (K-12 curriculum ê¸°ë°˜)
- âœ… Multi-label soft assignment
- âœ… Cross-cutting concept quantification
- âœ… Educational quality metrics

### 3. ì‹œê°„ ê´€ë¦¬
- Week 4/16 = 25% ì§„í–‰
- Subtask 1ë§Œ ì™„ë£Œí•˜ê¸°ì—ë„ ë¹ ë“¯
- Subtask 2 (model training)ëŠ” Week 11+ ì´í›„ í˜„ì‹¤ì 
- Subtask 3 (refinement)ëŠ” "Future Work"ë¡œ ì²˜ë¦¬ ê°€ëŠ¥

---

## ğŸ¤” ì—¬ì „íˆ ë¯¸í•´ê²°ëœ ì§ˆë¬¸ë“¤

1. **Validation**: Domain classification accuracyëŠ” ì–¼ë§ˆë‚˜ ë˜ëŠ”ê°€?
   - Manual labelingìœ¼ë¡œ ground truth ìƒì„± í•„ìš”
   - 100ê°œ sampleë¡œ precision/recall ì¸¡ì •

2. **Threshold ì„ íƒ**: `MIN_SIMILARITY = 0.3`ì´ ì ì ˆí•œê°€?
   - Too high â†’ many unlabeled paragraphs
   - Too low â†’ noisy labels
   - Validation í›„ ì¡°ì • í•„ìš”

3. **The Pile í™•ì¥**: ì–´ë–»ê²Œ ìƒ˜í”Œë§í•  ê²ƒì¸ê°€?
   - Stratified by subset? (ArXiv, StackExchange, Books3, etc.)
   - Random 5GB sample?
   - ì‹œê°„ ì œì•½ ê³ ë ¤í•´ì•¼ í•¨

4. **Prerequisite ê´€ê³„**: ì—¬ì „íˆ í•„ìš”í•œê°€?
   - Curriculum orderingì—ëŠ” í•„ìš”
   - í•˜ì§€ë§Œ co-occurrence ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨íˆ ì²˜ë¦¬ ê°€ëŠ¥
   - Week 8-9ì— ì¬ë…¼ì˜

---

## ğŸ“ˆ ì§„í–‰ë¥  ì‹œê°í™”

```
Week 1-3: [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] Data collection (ì™„ë£Œ)
Week 4:   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Subtask 1 - Metrics pipeline (80% ì™„ë£Œ)
Week 5:   [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Full analysis + validation (ì˜ˆì •)
Week 6:   [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Difficulty + redundancy metrics (ì˜ˆì •)
Week 7-8: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] The Pile sampling + analysis (ì˜ˆì •)
Week 9-10:[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Prerequisite mining (ì˜ˆì •)
Week 11+: [â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] Subtask 2 - Model training (ë¯¸ì •)
```

---

## ğŸ“ ì˜ˆìƒ Contribution

**Conference Target**: COLM 2026 (August deadline) or EMNLP 2026

**Paper Angle**:
> "Fine-Grained Domain Characterization of Pretraining Corpora for Curriculum-Aware SLM Training"

**Key Claims**:
1. Existing datasets lack fine-grained domain analysis
2. Educational taxonomies can guide curriculum learning
3. Cross-cutting concepts are prevalent and measurable
4. Domain-balanced data improves SLM efficiency (if Subtask 2 ì™„ë£Œ ì‹œ)

**Without Subtask 2**: Workshop paper or short paper
**With Subtask 2**: Full conference paper

---

## ğŸ”— ìœ ìš©í•œ ë§í¬

- **Notion Page**: https://www.notion.so/Phase-1-2f5fa6116ae180a2bf73ccd81ad7ae8e
- **Khan Academy ToS**: https://www.khanacademy.org/about/tos
- **Tiny-Textbooks Dataset**: https://huggingface.co/datasets/nampdn-ai/tiny-textbooks
- **SentenceTransformers Docs**: https://www.sbert.net/

---

## âœ… Action Items for Tomorrow

1. [ ] Step 1 ì‹¤í–‰ (taxonomy extraction)
2. [ ] Step 2 í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (`max_batches=5`)
3. [ ] Dashboard ìƒì„± ë° í™•ì¸
4. [ ] Manual validation ìƒ˜í”Œ 100ê°œ ì„ ì •
5. [ ] Professorì—ê²Œ ì§„í–‰ìƒí™© ì—…ë°ì´íŠ¸ ì´ë©”ì¼

---

**Last Updated**: 2026-02-10 19:30 KST
**Next Review**: 2026-02-11 (ë‚´ì¼)
